[
  {
    "objectID": "lab6.html",
    "href": "lab6.html",
    "title": "Lab 6: Machiene Learning",
    "section": "",
    "text": "library(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(tidymodels)\n\n── Attaching packages ────────────────────────────────────── tidymodels 1.2.0 ──\n✔ broom        1.0.7     ✔ rsample      1.2.1\n✔ dials        1.3.0     ✔ tune         1.2.1\n✔ infer        1.0.7     ✔ workflows    1.1.4\n✔ modeldata    1.4.0     ✔ workflowsets 1.1.0\n✔ parsnip      1.2.1     ✔ yardstick    1.3.1\n✔ recipes      1.1.0     \n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n✖ scales::discard() masks purrr::discard()\n✖ dplyr::filter()   masks stats::filter()\n✖ recipes::fixed()  masks stringr::fixed()\n✖ dplyr::lag()      masks stats::lag()\n✖ yardstick::spec() masks readr::spec()\n✖ recipes::step()   masks stats::step()\n• Dig deeper into tidy modeling with R at https://www.tmwr.org\n\nlibrary(powerjoin)\n\nWarning: package 'powerjoin' was built under R version 4.4.3\n\nlibrary(glue)\nlibrary(vip)\n\nWarning: package 'vip' was built under R version 4.4.3\n\n\n\nAttaching package: 'vip'\n\nThe following object is masked from 'package:utils':\n\n    vi\n\nlibrary(baguette)\n\nWarning: package 'baguette' was built under R version 4.4.3\n\nlibrary(ggthemes)\n\nWarning: package 'ggthemes' was built under R version 4.4.3\n\nlibrary(ggplot2)\nlibrary(sf)\n\nLinking to GEOS 3.13.0, GDAL 3.10.1, PROJ 9.5.1; sf_use_s2() is TRUE\n\nlibrary(patchwork)\nlibrary(xgboost)\n\nWarning: package 'xgboost' was built under R version 4.4.3\n\n\n\nAttaching package: 'xgboost'\n\nThe following object is masked from 'package:dplyr':\n\n    slice\nroot  &lt;- 'https://gdex.ucar.edu/dataset/camels/file'\ndownload.file('https://gdex.ucar.edu/dataset/camels/file/camels_attributes_v2.0.pdf', \n              'data/camels_attributes_v2.0.pdf')\ntypes &lt;- c(\"clim\", \"geol\", \"soil\", \"topo\", \"vege\", \"hydro\")\n# Where the files live online ...\nremote_files  &lt;- glue('{root}/camels_{types}.txt')\n# where we want to download the data ...\nlocal_files   &lt;- glue('data/camels_{types}.txt')\nwalk2(remote_files, local_files, download.file, quiet = TRUE)\n# Read and merge data\ncamels &lt;- map(local_files, read_delim, show_col_types = FALSE)\ncamels &lt;- power_full_join(camels ,by = 'gauge_id')"
  },
  {
    "objectID": "lab6.html#zero_r_freq-represents-the-frequency-of-zero-flow-days",
    "href": "lab6.html#zero_r_freq-represents-the-frequency-of-zero-flow-days",
    "title": "Lab 6: Machiene Learning",
    "section": "zero_r_freq represents the frequency of zero flow days",
    "text": "zero_r_freq represents the frequency of zero flow days\n\nggplot(data = camels, aes(x = gauge_lon, y = gauge_lat)) +\n  borders(\"state\", colour = \"gray50\") +\n  geom_point(aes(color = q_mean)) +\n  scale_color_gradient(low = \"pink\", high = \"dodgerblue\") +\n  ggthemes::theme_map()"
  },
  {
    "objectID": "lab6.html#data-splitting",
    "href": "lab6.html#data-splitting",
    "title": "Lab 6: Machiene Learning",
    "section": "Data Splitting",
    "text": "Data Splitting\n\nset.seed(123)\ncamels &lt;- camels |&gt; \n  mutate(logQmean = log(q_mean))\n\ncamels_split &lt;- initial_split(camels, prop = 0.75)\n\ncamels_train &lt;- training(camels_split)\ncamels_test  &lt;- testing(camels_split)\n\ncamels_cv &lt;- vfold_cv(camels_train, v = 10)\n\ndim(camels_train)\n\n[1] 503  59\n\ndim(camels_test)  \n\n[1] 168  59"
  },
  {
    "objectID": "lab6.html#recipe",
    "href": "lab6.html#recipe",
    "title": "Lab 6: Machiene Learning",
    "section": "Recipe",
    "text": "Recipe\n\np_mean + aridity + frac_snow = camels_train\n\n\nI chose precipitation, arditity, and snow because they directly affect water availability.\n\nrec &lt;- recipe(logQmean ~ aridity + p_mean + frac_snow, data = camels_train) %&gt;%\n  step_naomit(all_predictors(), all_outcomes()) %&gt;%  \n  step_log(all_numeric_predictors(), offset = 1e-6) %&gt;%  # \n  step_normalize(all_numeric_predictors()) %&gt;%  \n  step_interact(terms = ~ aridity:p_mean)"
  },
  {
    "objectID": "lab6.html#define-3-models",
    "href": "lab6.html#define-3-models",
    "title": "Lab 6: Machiene Learning",
    "section": "Define 3 models",
    "text": "Define 3 models\n\nModel 1: Random Forest\n\nrf_model &lt;- rand_forest(\n  mode = \"regression\",       \n  trees = 500               \n) %&gt;%\n  set_engine(\"ranger\")      \n\n\n\nModel 2: Linear Regression\n\nlm_model &lt;- linear_reg() %&gt;%\n  set_engine(\"lm\") %&gt;%\n  set_mode(\"regression\")\n\n\n\nModel 3: Gradient Boosting\n\ngb_model &lt;- boost_tree(\n  mode = \"regression\",\n  trees = 500,        \n  learn_rate = 0.1     \n) %&gt;%\n  set_engine(\"xgboost\")"
  },
  {
    "objectID": "lab6.html#workflow-set",
    "href": "lab6.html#workflow-set",
    "title": "Lab 6: Machiene Learning",
    "section": "Workflow set ()",
    "text": "Workflow set ()\n\nlibrary(tidymodels)\n\nwf_lm &lt;- workflow() %&gt;%\n  add_recipe(rec) %&gt;%\n  add_model(lm_model)\n\nwf_rf &lt;- workflow() %&gt;%\n  add_recipe(rec) %&gt;%\n  add_model(rf_model)\n\nwf_gb &lt;- workflow() %&gt;%\n  add_recipe(rec) %&gt;%\n  add_model(gb_model)\n\n\nwf_set &lt;- workflow_set(\n  preproc = list(rec), \n  models = list(lm_model, rf_model, gb_model) \n)\n\n\nwf_results &lt;- wf_set %&gt;%\n  workflow_map(\"fit_resamples\", resamples = camels_cv)"
  },
  {
    "objectID": "lab6.html#evaluation",
    "href": "lab6.html#evaluation",
    "title": "Lab 6: Machiene Learning",
    "section": "Evaluation",
    "text": "Evaluation\n\nautoplot(wf_results)\n\n\n\n\n\n\n\n\n\nrank_results(wf_results, rank_metric = \"rmse\")\n\n# A tibble: 6 × 9\n  wflow_id          .config .metric  mean std_err     n preprocessor model  rank\n  &lt;chr&gt;             &lt;chr&gt;   &lt;chr&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;int&gt; &lt;chr&gt;        &lt;chr&gt; &lt;int&gt;\n1 recipe_rand_fore… Prepro… rmse    0.433  0.0225    10 recipe       rand…     1\n2 recipe_rand_fore… Prepro… rsq     0.874  0.0122    10 recipe       rand…     1\n3 recipe_boost_tree Prepro… rmse    0.460  0.0363    10 recipe       boos…     2\n4 recipe_boost_tree Prepro… rsq     0.856  0.0177    10 recipe       boos…     2\n5 recipe_linear_reg Prepro… rmse    0.534  0.0284    10 recipe       line…     3\n6 recipe_linear_reg Prepro… rsq     0.802  0.0240    10 recipe       line…     3\n\n\n\nThe random forest model is the best because it has the lowest RMSE and highest R squared. It can also model complex relationships between variables, it captures interactions between variables, and handles missing data and outliers well."
  },
  {
    "objectID": "lab6.html#extract-and-evaluate",
    "href": "lab6.html#extract-and-evaluate",
    "title": "Lab 6: Machiene Learning",
    "section": "Extract and Evaluate",
    "text": "Extract and Evaluate\n\nfinal_wf &lt;- workflow() %&gt;%\n  add_recipe(rec) %&gt;%\n  add_model(rf_model)\n\n\nfinal_fit &lt;- final_wf %&gt;% fit(data = camels_train)\n\n\ntest_predictions &lt;- augment(final_fit, new_data = camels_test)\n\n\nggplot(test_predictions, aes(x = logQmean, y = .pred)) +\n  geom_point(alpha = 0.6, color = \"#1f77b4\") +  # Scatter plot with transparency\n  geom_abline(slope = 1, intercept = 0, color = \"red\", linetype = \"dashed\") + # 1:1 line\n  labs(\n    title = \"Observed vs. Predicted logQmean\",\n    x = \"Observed logQmean\",\n    y = \"Predicted logQmean\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nThe results seem strong, the points are mostly close to the 1:1 line indicating the model is performing well on the test data"
  }
]