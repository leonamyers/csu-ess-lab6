[
  {
    "objectID": "lab6.html",
    "href": "lab6.html",
    "title": "Lab 6: Machiene Learning",
    "section": "",
    "text": "library(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(tidymodels)\n\n── Attaching packages ────────────────────────────────────── tidymodels 1.2.0 ──\n✔ broom        1.0.7     ✔ rsample      1.2.1\n✔ dials        1.3.0     ✔ tune         1.2.1\n✔ infer        1.0.7     ✔ workflows    1.1.4\n✔ modeldata    1.4.0     ✔ workflowsets 1.1.0\n✔ parsnip      1.2.1     ✔ yardstick    1.3.1\n✔ recipes      1.1.0     \n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n✖ scales::discard() masks purrr::discard()\n✖ dplyr::filter()   masks stats::filter()\n✖ recipes::fixed()  masks stringr::fixed()\n✖ dplyr::lag()      masks stats::lag()\n✖ yardstick::spec() masks readr::spec()\n✖ recipes::step()   masks stats::step()\n• Search for functions across packages at https://www.tidymodels.org/find/\n\nlibrary(powerjoin)\n\nWarning: package 'powerjoin' was built under R version 4.4.3\n\nlibrary(glue)\nlibrary(vip)\n\nWarning: package 'vip' was built under R version 4.4.3\n\n\n\nAttaching package: 'vip'\n\nThe following object is masked from 'package:utils':\n\n    vi\n\nlibrary(baguette)\n\nWarning: package 'baguette' was built under R version 4.4.3\n\nlibrary(ggthemes)\n\nWarning: package 'ggthemes' was built under R version 4.4.3\n\nlibrary(ggplot2)\nlibrary(sf)\n\nLinking to GEOS 3.13.0, GDAL 3.10.1, PROJ 9.5.1; sf_use_s2() is TRUE\n\nlibrary(patchwork)\nlibrary(xgboost)\n\nWarning: package 'xgboost' was built under R version 4.4.3\n\n\n\nAttaching package: 'xgboost'\n\nThe following object is masked from 'package:dplyr':\n\n    slice\nroot  &lt;- 'https://gdex.ucar.edu/dataset/camels/file'\ndownload.file('https://gdex.ucar.edu/dataset/camels/file/camels_attributes_v2.0.pdf', \n              'data/camels_attributes_v2.0.pdf')\ntypes &lt;- c(\"clim\", \"geol\", \"soil\", \"topo\", \"vege\", \"hydro\")\n# Where the files live online ...\nremote_files  &lt;- glue('{root}/camels_{types}.txt')\n# where we want to download the data ...\nlocal_files   &lt;- glue('data/camels_{types}.txt')\nwalk2(remote_files, local_files, download.file, quiet = TRUE)\n# Read and merge data\ncamels &lt;- map(local_files, read_delim, show_col_types = FALSE)\ncamels &lt;- power_full_join(camels ,by = 'gauge_id')"
  },
  {
    "objectID": "lab6.html#zero_r_freq-represents-the-frequency-of-zero-flow-days",
    "href": "lab6.html#zero_r_freq-represents-the-frequency-of-zero-flow-days",
    "title": "Lab 6: Machiene Learning",
    "section": "zero_r_freq represents the frequency of zero flow days",
    "text": "zero_r_freq represents the frequency of zero flow days\n\nggplot(data = camels, aes(x = gauge_lon, y = gauge_lat)) +\n  borders(\"state\", colour = \"gray50\") +\n  geom_point(aes(color = q_mean)) +\n  scale_color_gradient(low = \"pink\", high = \"dodgerblue\") +\n  ggthemes::theme_map()"
  },
  {
    "objectID": "lab6.html#data-splitting",
    "href": "lab6.html#data-splitting",
    "title": "Lab 6: Machiene Learning",
    "section": "Data Splitting",
    "text": "Data Splitting\n\nset.seed(123)\ncamels &lt;- camels |&gt; \n  mutate(logQmean = log(q_mean))\n\ncamels_split &lt;- initial_split(camels, prop = 0.75)\n\ncamels_train &lt;- training(camels_split)\ncamels_test  &lt;- testing(camels_split)\n\ncamels_cv &lt;- vfold_cv(camels_train, v = 10)\n\ndim(camels_train)\n\n[1] 503  59\n\ndim(camels_test)  \n\n[1] 168  59"
  },
  {
    "objectID": "lab6.html#recipe",
    "href": "lab6.html#recipe",
    "title": "Lab 6: Machiene Learning",
    "section": "Recipe",
    "text": "Recipe\n\np_mean + aridity + frac_snow = camels_train\n\n\nI chose precipitation, arditity, and snow because they directly affect water availability.\n\nrec &lt;- recipe(logQmean ~ aridity + p_mean + frac_snow, data = camels_train) %&gt;%\n  step_naomit(all_predictors(), all_outcomes()) %&gt;%  \n  step_log(all_numeric_predictors(), offset = 1e-6) %&gt;%  # \n  step_normalize(all_numeric_predictors()) %&gt;%  \n  step_interact(terms = ~ aridity:p_mean)"
  },
  {
    "objectID": "lab6.html#define-3-models",
    "href": "lab6.html#define-3-models",
    "title": "Lab 6: Machiene Learning",
    "section": "Define 3 models",
    "text": "Define 3 models\n\nModel 1: Random Forest\n\nrf_model &lt;- rand_forest(\n  mode = \"regression\",       \n  trees = 500               \n) %&gt;%\n  set_engine(\"ranger\")      \n\n\n\nModel 2: Linear Regression\n\nlm_model &lt;- linear_reg() %&gt;%\n  set_engine(\"lm\") %&gt;%\n  set_mode(\"regression\")\n\n\n\nModel 3: Gradient Boosting\n\ngb_model &lt;- boost_tree(\n  mode = \"regression\",\n  trees = 500,        \n  learn_rate = 0.1     \n) %&gt;%\n  set_engine(\"xgboost\")"
  },
  {
    "objectID": "lab6.html#workflow-set",
    "href": "lab6.html#workflow-set",
    "title": "Lab 6: Machiene Learning",
    "section": "Workflow set ()",
    "text": "Workflow set ()\n\nlibrary(tidymodels)\n\nwf_lm &lt;- workflow() %&gt;%\n  add_recipe(rec) %&gt;%\n  add_model(lm_model)\n\nwf_rf &lt;- workflow() %&gt;%\n  add_recipe(rec) %&gt;%\n  add_model(rf_model)\n\nwf_gb &lt;- workflow() %&gt;%\n  add_recipe(rec) %&gt;%\n  add_model(gb_model)\n\n\nwf_set &lt;- workflow_set(\n  preproc = list(rec), \n  models = list(lm_model, rf_model, gb_model) \n)\n\n\nwf_results &lt;- wf_set %&gt;%\n  workflow_map(\"fit_resamples\", resamples = camels_cv)"
  },
  {
    "objectID": "lab6.html#evaluation",
    "href": "lab6.html#evaluation",
    "title": "Lab 6: Machiene Learning",
    "section": "Evaluation",
    "text": "Evaluation\n\nautoplot(wf_results)\n\n\n\n\n\n\n\n\n\nrank_results(wf_results, rank_metric = \"rmse\")\n\n# A tibble: 6 × 9\n  wflow_id          .config .metric  mean std_err     n preprocessor model  rank\n  &lt;chr&gt;             &lt;chr&gt;   &lt;chr&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;int&gt; &lt;chr&gt;        &lt;chr&gt; &lt;int&gt;\n1 recipe_rand_fore… Prepro… rmse    0.433  0.0225    10 recipe       rand…     1\n2 recipe_rand_fore… Prepro… rsq     0.874  0.0122    10 recipe       rand…     1\n3 recipe_boost_tree Prepro… rmse    0.460  0.0363    10 recipe       boos…     2\n4 recipe_boost_tree Prepro… rsq     0.856  0.0177    10 recipe       boos…     2\n5 recipe_linear_reg Prepro… rmse    0.534  0.0284    10 recipe       line…     3\n6 recipe_linear_reg Prepro… rsq     0.802  0.0240    10 recipe       line…     3\n\n\n\nThe random forest model is the best because it has the lowest RMSE and highest R squared. It can also model complex relationships between variables, it captures interactions between variables, and handles missing data and outliers well."
  },
  {
    "objectID": "lab6.html#extract-and-evaluate",
    "href": "lab6.html#extract-and-evaluate",
    "title": "Lab 6: Machiene Learning",
    "section": "Extract and Evaluate",
    "text": "Extract and Evaluate\n\nfinal_wf &lt;- workflow() %&gt;%\n  add_recipe(rec) %&gt;%\n  add_model(rf_model)\n\n\nfinal_fit &lt;- final_wf %&gt;% fit(data = camels_train)\n\n\ntest_predictions &lt;- augment(final_fit, new_data = camels_test)\n\n\nggplot(test_predictions, aes(x = logQmean, y = .pred)) +\n  geom_point(alpha = 0.6, color = \"#1f77b4\") +  # Scatter plot with transparency\n  geom_abline(slope = 1, intercept = 0, color = \"red\", linetype = \"dashed\") + # 1:1 line\n  labs(\n    title = \"Observed vs. Predicted logQmean\",\n    x = \"Observed logQmean\",\n    y = \"Predicted logQmean\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nThe results seem strong, the points are mostly close to the 1:1 line indicating the model is performing well on the test data"
  },
  {
    "objectID": "hyperparameter-tuning.html",
    "href": "hyperparameter-tuning.html",
    "title": "hyperparameter-tuning",
    "section": "",
    "text": "library(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(tidymodels)\n\n── Attaching packages ────────────────────────────────────── tidymodels 1.2.0 ──\n✔ broom        1.0.7     ✔ rsample      1.2.1\n✔ dials        1.3.0     ✔ tune         1.2.1\n✔ infer        1.0.7     ✔ workflows    1.1.4\n✔ modeldata    1.4.0     ✔ workflowsets 1.1.0\n✔ parsnip      1.2.1     ✔ yardstick    1.3.1\n✔ recipes      1.1.0     \n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n✖ scales::discard() masks purrr::discard()\n✖ dplyr::filter()   masks stats::filter()\n✖ recipes::fixed()  masks stringr::fixed()\n✖ dplyr::lag()      masks stats::lag()\n✖ yardstick::spec() masks readr::spec()\n✖ recipes::step()   masks stats::step()\n• Use tidymodels_prefer() to resolve common conflicts.\n\nlibrary(powerjoin)\n\nWarning: package 'powerjoin' was built under R version 4.4.3\n\nlibrary(skimr)\n\nWarning: package 'skimr' was built under R version 4.4.3\n\nlibrary(visdat)\nlibrary(ggpubr)\n\nWarning: package 'ggpubr' was built under R version 4.4.3\n\nlibrary(glue)"
  },
  {
    "objectID": "hyperparameter-tuning.html#libraries",
    "href": "hyperparameter-tuning.html#libraries",
    "title": "hyperparameter-tuning",
    "section": "",
    "text": "library(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(tidymodels)\n\n── Attaching packages ────────────────────────────────────── tidymodels 1.2.0 ──\n✔ broom        1.0.7     ✔ rsample      1.2.1\n✔ dials        1.3.0     ✔ tune         1.2.1\n✔ infer        1.0.7     ✔ workflows    1.1.4\n✔ modeldata    1.4.0     ✔ workflowsets 1.1.0\n✔ parsnip      1.2.1     ✔ yardstick    1.3.1\n✔ recipes      1.1.0     \n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n✖ scales::discard() masks purrr::discard()\n✖ dplyr::filter()   masks stats::filter()\n✖ recipes::fixed()  masks stringr::fixed()\n✖ dplyr::lag()      masks stats::lag()\n✖ yardstick::spec() masks readr::spec()\n✖ recipes::step()   masks stats::step()\n• Use tidymodels_prefer() to resolve common conflicts.\n\nlibrary(powerjoin)\n\nWarning: package 'powerjoin' was built under R version 4.4.3\n\nlibrary(skimr)\n\nWarning: package 'skimr' was built under R version 4.4.3\n\nlibrary(visdat)\nlibrary(ggpubr)\n\nWarning: package 'ggpubr' was built under R version 4.4.3\n\nlibrary(glue)"
  },
  {
    "objectID": "hyperparameter-tuning.html#data-importtidytransform",
    "href": "hyperparameter-tuning.html#data-importtidytransform",
    "title": "hyperparameter-tuning",
    "section": "Data Import/Tidy/Transform",
    "text": "Data Import/Tidy/Transform\n\nroot  &lt;- 'https://gdex.ucar.edu/dataset/camels/file'\n\n\ndownload.file('https://gdex.ucar.edu/dataset/camels/file/camels_attributes_v2.0.pdf', \n              'data/camels_attributes_v2.0.pdf')\n\n\ntypes &lt;- c(\"clim\", \"geol\", \"soil\", \"topo\", \"vege\", \"hydro\")\n\n\nremote_files  &lt;- glue('{root}/camels_{types}.txt')\n\nlocal_files   &lt;- glue('data/camels_{types}.txt')\n\n\nwalk2(remote_files, local_files, download.file, quiet = TRUE)\n\n\ncamels &lt;- map(local_files, read_delim, show_col_types = FALSE) \n\n\ncamels &lt;- power_full_join(camels ,by = 'gauge_id')\n\n\nglimpse(camels)\n\nRows: 671\nColumns: 58\n$ gauge_id             &lt;chr&gt; \"01013500\", \"01022500\", \"01030500\", \"01031500\", \"…\n$ p_mean               &lt;dbl&gt; 3.126679, 3.608126, 3.274405, 3.522957, 3.323146,…\n$ pet_mean             &lt;dbl&gt; 1.971555, 2.119256, 2.043594, 2.071324, 2.090024,…\n$ p_seasonality        &lt;dbl&gt; 0.187940259, -0.114529586, 0.047358189, 0.1040905…\n$ frac_snow            &lt;dbl&gt; 0.3134404, 0.2452590, 0.2770184, 0.2918365, 0.280…\n$ aridity              &lt;dbl&gt; 0.6305587, 0.5873564, 0.6241114, 0.5879503, 0.628…\n$ high_prec_freq       &lt;dbl&gt; 12.95, 20.55, 17.15, 18.90, 20.10, 13.50, 17.50, …\n$ high_prec_dur        &lt;dbl&gt; 1.348958, 1.205279, 1.207746, 1.148936, 1.165217,…\n$ high_prec_timing     &lt;chr&gt; \"son\", \"son\", \"son\", \"son\", \"son\", \"jja\", \"son\", …\n$ low_prec_freq        &lt;dbl&gt; 202.20, 233.65, 215.60, 227.35, 235.90, 193.50, 2…\n$ low_prec_dur         &lt;dbl&gt; 3.427119, 3.662226, 3.514262, 3.473644, 3.691706,…\n$ low_prec_timing      &lt;chr&gt; \"mam\", \"jja\", \"djf\", \"djf\", \"djf\", \"mam\", \"mam\", …\n$ geol_1st_class       &lt;chr&gt; \"Siliciclastic sedimentary rocks\", \"Acid plutonic…\n$ glim_1st_class_frac  &lt;dbl&gt; 0.8159044, 0.5906582, 0.5733054, 0.4489279, 0.308…\n$ geol_2nd_class       &lt;chr&gt; \"Basic volcanic rocks\", \"Siliciclastic sedimentar…\n$ glim_2nd_class_frac  &lt;dbl&gt; 0.17972945, 0.16461821, 0.28701001, 0.44386282, 0…\n$ carbonate_rocks_frac &lt;dbl&gt; 0.000000000, 0.000000000, 0.052140094, 0.02625797…\n$ geol_porostiy        &lt;dbl&gt; 0.1714, 0.0710, 0.1178, 0.0747, 0.0522, 0.0711, 0…\n$ geol_permeability    &lt;dbl&gt; -14.7019, -14.2138, -14.4918, -14.8410, -14.4819,…\n$ soil_depth_pelletier &lt;dbl&gt; 7.4047619, 17.4128079, 19.0114144, 7.2525570, 5.3…\n$ soil_depth_statsgo   &lt;dbl&gt; 1.248408, 1.491846, 1.461363, 1.279047, 1.392779,…\n$ soil_porosity        &lt;dbl&gt; 0.4611488, 0.4159055, 0.4590910, 0.4502360, 0.422…\n$ soil_conductivity    &lt;dbl&gt; 1.106522, 2.375005, 1.289807, 1.373292, 2.615154,…\n$ max_water_content    &lt;dbl&gt; 0.5580548, 0.6262289, 0.6530198, 0.5591227, 0.561…\n$ sand_frac            &lt;dbl&gt; 27.84183, 59.39016, 32.23546, 35.26903, 55.16313,…\n$ silt_frac            &lt;dbl&gt; 55.15694, 28.08094, 51.77918, 50.84123, 34.18544,…\n$ clay_frac            &lt;dbl&gt; 16.275732, 12.037646, 14.776824, 12.654125, 10.30…\n$ water_frac           &lt;dbl&gt; 5.3766978, 1.2269127, 1.6343449, 0.6745936, 0.000…\n$ organic_frac         &lt;dbl&gt; 0.4087168, 0.0000000, 1.3302776, 0.0000000, 0.000…\n$ other_frac           &lt;dbl&gt; 0.0000000, 0.3584723, 0.0220161, 0.0000000, 0.147…\n$ gauge_lat            &lt;dbl&gt; 47.23739, 44.60797, 45.50097, 45.17501, 44.86920,…\n$ gauge_lon            &lt;dbl&gt; -68.58264, -67.93524, -68.30596, -69.31470, -69.9…\n$ elev_mean            &lt;dbl&gt; 250.31, 92.68, 143.80, 247.80, 310.38, 615.70, 47…\n$ slope_mean           &lt;dbl&gt; 21.64152, 17.79072, 12.79195, 29.56035, 49.92122,…\n$ area_gages2          &lt;dbl&gt; 2252.70, 573.60, 3676.17, 769.05, 909.10, 383.82,…\n$ area_geospa_fabric   &lt;dbl&gt; 2303.95, 620.38, 3676.09, 766.53, 904.94, 396.10,…\n$ frac_forest          &lt;dbl&gt; 0.9063, 0.9232, 0.8782, 0.9548, 0.9906, 1.0000, 1…\n$ lai_max              &lt;dbl&gt; 4.167304, 4.871392, 4.685200, 4.903259, 5.086811,…\n$ lai_diff             &lt;dbl&gt; 3.340732, 3.746692, 3.665543, 3.990843, 4.300978,…\n$ gvf_max              &lt;dbl&gt; 0.8045674, 0.8639358, 0.8585020, 0.8706685, 0.891…\n$ gvf_diff             &lt;dbl&gt; 0.3716482, 0.3377125, 0.3513934, 0.3986194, 0.445…\n$ dom_land_cover_frac  &lt;dbl&gt; 0.8834519, 0.8204934, 0.9752580, 1.0000000, 0.850…\n$ dom_land_cover       &lt;chr&gt; \"    Mixed Forests\", \"    Mixed Forests\", \"    Mi…\n$ root_depth_50        &lt;dbl&gt; NA, 0.2374345, NA, 0.2500000, 0.2410270, 0.225615…\n$ root_depth_99        &lt;dbl&gt; NA, 2.238444, NA, 2.400000, 2.340180, 2.237435, 2…\n$ q_mean               &lt;dbl&gt; 1.699155, 2.173062, 1.820108, 2.030242, 2.182870,…\n$ runoff_ratio         &lt;dbl&gt; 0.5434375, 0.6022689, 0.5558590, 0.5762893, 0.656…\n$ slope_fdc            &lt;dbl&gt; 1.528219, 1.776280, 1.871110, 1.494019, 1.415939,…\n$ baseflow_index       &lt;dbl&gt; 0.5852260, 0.5544784, 0.5084407, 0.4450905, 0.473…\n$ stream_elas          &lt;dbl&gt; 1.8453242, 1.7027824, 1.3775052, 1.6486930, 1.510…\n$ q5                   &lt;dbl&gt; 0.24110613, 0.20473436, 0.10714920, 0.11134535, 0…\n$ q95                  &lt;dbl&gt; 6.373021, 7.123049, 6.854887, 8.010503, 8.095148,…\n$ high_q_freq          &lt;dbl&gt; 6.10, 3.90, 12.25, 18.90, 14.95, 14.10, 16.05, 16…\n$ high_q_dur           &lt;dbl&gt; 8.714286, 2.294118, 7.205882, 3.286957, 2.577586,…\n$ low_q_freq           &lt;dbl&gt; 41.35, 65.15, 89.25, 94.80, 71.55, 58.90, 82.20, …\n$ low_q_dur            &lt;dbl&gt; 20.170732, 17.144737, 19.402174, 14.697674, 12.77…\n$ zero_q_freq          &lt;dbl&gt; 0.00000000, 0.00000000, 0.00000000, 0.00000000, 0…\n$ hfd_mean             &lt;dbl&gt; 207.25, 166.25, 184.90, 181.00, 184.80, 197.20, 1…\n\nskim(camels)\n\n\nData summary\n\n\nName\ncamels\n\n\nNumber of rows\n671\n\n\nNumber of columns\n58\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n6\n\n\nnumeric\n52\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\ngauge_id\n0\n1.00\n8\n8\n0\n671\n0\n\n\nhigh_prec_timing\n0\n1.00\n3\n3\n0\n4\n0\n\n\nlow_prec_timing\n0\n1.00\n3\n3\n0\n4\n0\n\n\ngeol_1st_class\n0\n1.00\n12\n31\n0\n12\n0\n\n\ngeol_2nd_class\n138\n0.79\n12\n31\n0\n13\n0\n\n\ndom_land_cover\n0\n1.00\n12\n38\n0\n12\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\np_mean\n0\n1.00\n3.26\n1.41\n0.64\n2.37\n3.23\n3.78\n8.94\n▃▇▂▁▁\n\n\npet_mean\n0\n1.00\n2.79\n0.55\n1.90\n2.34\n2.69\n3.15\n4.74\n▇▇▅▂▁\n\n\np_seasonality\n0\n1.00\n-0.04\n0.53\n-1.44\n-0.26\n0.08\n0.22\n0.92\n▁▂▃▇▂\n\n\nfrac_snow\n0\n1.00\n0.18\n0.20\n0.00\n0.04\n0.10\n0.22\n0.91\n▇▂▁▁▁\n\n\naridity\n0\n1.00\n1.06\n0.62\n0.22\n0.70\n0.86\n1.27\n5.21\n▇▂▁▁▁\n\n\nhigh_prec_freq\n0\n1.00\n20.93\n4.55\n7.90\n18.50\n22.00\n24.23\n32.70\n▂▃▇▇▁\n\n\nhigh_prec_dur\n0\n1.00\n1.35\n0.19\n1.08\n1.21\n1.28\n1.44\n2.09\n▇▅▂▁▁\n\n\nlow_prec_freq\n0\n1.00\n254.65\n35.12\n169.90\n232.70\n255.85\n278.92\n348.70\n▂▅▇▅▁\n\n\nlow_prec_dur\n0\n1.00\n5.95\n3.20\n2.79\n4.24\n4.95\n6.70\n36.51\n▇▁▁▁▁\n\n\nglim_1st_class_frac\n0\n1.00\n0.79\n0.20\n0.30\n0.61\n0.83\n1.00\n1.00\n▁▃▃▃▇\n\n\nglim_2nd_class_frac\n0\n1.00\n0.16\n0.14\n0.00\n0.00\n0.14\n0.27\n0.49\n▇▃▃▂▁\n\n\ncarbonate_rocks_frac\n0\n1.00\n0.12\n0.26\n0.00\n0.00\n0.00\n0.04\n1.00\n▇▁▁▁▁\n\n\ngeol_porostiy\n3\n1.00\n0.13\n0.07\n0.01\n0.07\n0.13\n0.19\n0.28\n▇▆▇▇▂\n\n\ngeol_permeability\n0\n1.00\n-13.89\n1.18\n-16.50\n-14.77\n-13.96\n-13.00\n-10.90\n▂▅▇▅▂\n\n\nsoil_depth_pelletier\n0\n1.00\n10.87\n16.24\n0.27\n1.00\n1.23\n12.89\n50.00\n▇▁▁▁▁\n\n\nsoil_depth_statsgo\n0\n1.00\n1.29\n0.27\n0.40\n1.11\n1.46\n1.50\n1.50\n▁▁▂▂▇\n\n\nsoil_porosity\n0\n1.00\n0.44\n0.02\n0.37\n0.43\n0.44\n0.46\n0.68\n▃▇▁▁▁\n\n\nsoil_conductivity\n0\n1.00\n1.74\n1.52\n0.45\n0.93\n1.35\n1.93\n13.96\n▇▁▁▁▁\n\n\nmax_water_content\n0\n1.00\n0.53\n0.15\n0.09\n0.43\n0.56\n0.64\n1.05\n▁▅▇▃▁\n\n\nsand_frac\n0\n1.00\n36.47\n15.63\n8.18\n25.44\n35.27\n44.46\n91.98\n▅▇▅▁▁\n\n\nsilt_frac\n0\n1.00\n33.86\n13.25\n2.99\n23.95\n34.06\n43.64\n67.77\n▂▆▇▆▁\n\n\nclay_frac\n0\n1.00\n19.89\n9.32\n1.85\n14.00\n18.66\n25.42\n50.35\n▃▇▅▂▁\n\n\nwater_frac\n0\n1.00\n0.10\n0.94\n0.00\n0.00\n0.00\n0.00\n19.35\n▇▁▁▁▁\n\n\norganic_frac\n0\n1.00\n0.59\n3.84\n0.00\n0.00\n0.00\n0.00\n57.86\n▇▁▁▁▁\n\n\nother_frac\n0\n1.00\n9.82\n16.83\n0.00\n0.00\n1.31\n11.74\n99.38\n▇▁▁▁▁\n\n\ngauge_lat\n0\n1.00\n39.24\n5.21\n27.05\n35.70\n39.25\n43.21\n48.82\n▂▃▇▆▅\n\n\ngauge_lon\n0\n1.00\n-95.79\n16.21\n-124.39\n-110.41\n-92.78\n-81.77\n-67.94\n▆▃▇▇▅\n\n\nelev_mean\n0\n1.00\n759.42\n786.00\n10.21\n249.67\n462.72\n928.88\n3571.18\n▇▂▁▁▁\n\n\nslope_mean\n0\n1.00\n46.20\n47.12\n0.82\n7.43\n28.80\n73.17\n255.69\n▇▂▂▁▁\n\n\narea_gages2\n0\n1.00\n792.62\n1701.95\n4.03\n122.28\n329.68\n794.30\n25791.04\n▇▁▁▁▁\n\n\narea_geospa_fabric\n0\n1.00\n808.08\n1709.85\n4.10\n127.98\n340.70\n804.50\n25817.78\n▇▁▁▁▁\n\n\nfrac_forest\n0\n1.00\n0.64\n0.37\n0.00\n0.28\n0.81\n0.97\n1.00\n▃▁▁▂▇\n\n\nlai_max\n0\n1.00\n3.22\n1.52\n0.37\n1.81\n3.37\n4.70\n5.58\n▅▆▃▅▇\n\n\nlai_diff\n0\n1.00\n2.45\n1.33\n0.15\n1.20\n2.34\n3.76\n4.83\n▇▇▇▆▇\n\n\ngvf_max\n0\n1.00\n0.72\n0.17\n0.18\n0.61\n0.78\n0.86\n0.92\n▁▁▂▃▇\n\n\ngvf_diff\n0\n1.00\n0.32\n0.15\n0.03\n0.19\n0.32\n0.46\n0.65\n▃▇▅▇▁\n\n\ndom_land_cover_frac\n0\n1.00\n0.81\n0.18\n0.31\n0.65\n0.86\n1.00\n1.00\n▁▂▃▃▇\n\n\nroot_depth_50\n24\n0.96\n0.18\n0.03\n0.12\n0.17\n0.18\n0.19\n0.25\n▃▃▇▂▂\n\n\nroot_depth_99\n24\n0.96\n1.83\n0.30\n1.50\n1.52\n1.80\n2.00\n3.10\n▇▃▂▁▁\n\n\nq_mean\n1\n1.00\n1.49\n1.54\n0.00\n0.63\n1.13\n1.75\n9.69\n▇▁▁▁▁\n\n\nrunoff_ratio\n1\n1.00\n0.39\n0.23\n0.00\n0.24\n0.35\n0.51\n1.36\n▆▇▂▁▁\n\n\nslope_fdc\n1\n1.00\n1.24\n0.51\n0.00\n0.90\n1.28\n1.63\n2.50\n▂▅▇▇▁\n\n\nbaseflow_index\n0\n1.00\n0.49\n0.16\n0.01\n0.40\n0.50\n0.60\n0.98\n▁▃▇▅▁\n\n\nstream_elas\n1\n1.00\n1.83\n0.78\n-0.64\n1.32\n1.70\n2.23\n6.24\n▁▇▃▁▁\n\n\nq5\n1\n1.00\n0.17\n0.27\n0.00\n0.01\n0.08\n0.22\n2.42\n▇▁▁▁▁\n\n\nq95\n1\n1.00\n5.06\n4.94\n0.00\n2.07\n3.77\n6.29\n31.82\n▇▂▁▁▁\n\n\nhigh_q_freq\n1\n1.00\n25.74\n29.07\n0.00\n6.41\n15.10\n35.79\n172.80\n▇▂▁▁▁\n\n\nhigh_q_dur\n1\n1.00\n6.91\n10.07\n0.00\n1.82\n2.85\n7.55\n92.56\n▇▁▁▁▁\n\n\nlow_q_freq\n1\n1.00\n107.62\n82.24\n0.00\n37.44\n96.00\n162.14\n356.80\n▇▆▅▂▁\n\n\nlow_q_dur\n1\n1.00\n22.28\n21.66\n0.00\n10.00\n15.52\n26.91\n209.88\n▇▁▁▁▁\n\n\nzero_q_freq\n1\n1.00\n0.03\n0.11\n0.00\n0.00\n0.00\n0.00\n0.97\n▇▁▁▁▁\n\n\nhfd_mean\n1\n1.00\n182.52\n33.53\n112.25\n160.16\n173.77\n204.05\n287.75\n▂▇▃▂▁\n\n\n\n\nvis_dat(camels)\n\n\n\n\n\n\n\n\n\ncamels_clean &lt;- camels %&gt;%\n  select(where(~!all(is.na(.))))\n\ncamels_clean &lt;- camels %&gt;%\n  filter(!is.na(q_mean))\n\ncamels_clean %&gt;%\n  summarise(across(everything(), ~sum(is.na(.))))\n\n# A tibble: 1 × 58\n  gauge_id p_mean pet_mean p_seasonality frac_snow aridity high_prec_freq\n     &lt;int&gt;  &lt;int&gt;    &lt;int&gt;         &lt;int&gt;     &lt;int&gt;   &lt;int&gt;          &lt;int&gt;\n1        0      0        0             0         0       0              0\n# ℹ 51 more variables: high_prec_dur &lt;int&gt;, high_prec_timing &lt;int&gt;,\n#   low_prec_freq &lt;int&gt;, low_prec_dur &lt;int&gt;, low_prec_timing &lt;int&gt;,\n#   geol_1st_class &lt;int&gt;, glim_1st_class_frac &lt;int&gt;, geol_2nd_class &lt;int&gt;,\n#   glim_2nd_class_frac &lt;int&gt;, carbonate_rocks_frac &lt;int&gt;, geol_porostiy &lt;int&gt;,\n#   geol_permeability &lt;int&gt;, soil_depth_pelletier &lt;int&gt;,\n#   soil_depth_statsgo &lt;int&gt;, soil_porosity &lt;int&gt;, soil_conductivity &lt;int&gt;,\n#   max_water_content &lt;int&gt;, sand_frac &lt;int&gt;, silt_frac &lt;int&gt;, …"
  },
  {
    "objectID": "hyperparameter-tuning.html#data-spliting",
    "href": "hyperparameter-tuning.html#data-spliting",
    "title": "hyperparameter-tuning",
    "section": "Data Spliting",
    "text": "Data Spliting\n\nset.seed(123)\n\ncamels_split &lt;- initial_split(camels_clean, prop = 0.8)\n\ncamels_train &lt;- training(camels_split)\ncamels_test  &lt;- testing(camels_split)"
  },
  {
    "objectID": "hyperparameter-tuning.html#feature-engineering",
    "href": "hyperparameter-tuning.html#feature-engineering",
    "title": "hyperparameter-tuning",
    "section": "Feature Engineering",
    "text": "Feature Engineering\n\nlibrary(recipes)\n\ncamels_recipe &lt;- recipe(q_mean ~ ., data = camels_train) %&gt;%\n  step_rm(gauge_lat, gauge_lon, gauge_id) %&gt;%\n  step_naomit(all_predictors()) %&gt;%\n  step_novel(all_nominal_predictors()) %&gt;%          \n  step_unknown(all_nominal_predictors()) %&gt;%  \n  step_other(all_nominal_predictors(), threshold = 0.01) %&gt;%\n  step_dummy(all_nominal_predictors()) %&gt;%\n  step_zv(all_predictors()) %&gt;%\n  step_normalize(all_numeric_predictors())"
  },
  {
    "objectID": "hyperparameter-tuning.html#resampling-and-model-testing",
    "href": "hyperparameter-tuning.html#resampling-and-model-testing",
    "title": "hyperparameter-tuning",
    "section": "Resampling and Model Testing",
    "text": "Resampling and Model Testing\n\n1. Build resamples\n\nset.seed(123)\n\ncamels_folds &lt;- vfold_cv(camels_train, v = 10)\n\n\n\n2. Build 3 Candidtae Models\n\nlinear_mod &lt;- linear_reg() %&gt;%\n  set_engine(\"lm\") %&gt;%\n  set_mode(\"regression\")\n\n\nrf_mod &lt;- rand_forest(trees = 500) %&gt;%\n  set_engine(\"ranger\") %&gt;%\n  set_mode(\"regression\")\n\n\nboost_mod &lt;- boost_tree(trees = 1000, learn_rate = 0.01) %&gt;%\n  set_engine(\"xgboost\") %&gt;%\n  set_mode(\"regression\")\n\n\n\n3. Test the models\n\nlibrary(workflowsets)\n\ncamels_workflows &lt;- workflow_set(\n  preproc = list(camels_recipe = camels_recipe),\n  models = list(\n    linear = linear_mod,\n    random_forest = rf_mod,\n    boosted_tree = boost_mod\n  )\n)\n\n\nset.seed(123)\ncamels_results &lt;- camels_workflows %&gt;%\n  workflow_map(resamples = camels_folds,\n               control = control_resamples(save_pred = TRUE))\n\nWarning: package 'ranger' was built under R version 4.4.3\n\n\nWarning: package 'xgboost' was built under R version 4.4.3\n\n\n\nautoplot(camels_results)\n\n\n\n\n\n\n\n\n\n\n4. Model Selection\n\nThe linear regression model is the best choice for predicting q_mean in this dataset because of its low RMSE and high R squared.\n\n\nModel type: Linear regression\n\n\nEngine: lm\n\n\nMode: Regression"
  },
  {
    "objectID": "hyperparameter-tuning.html#model-tuning",
    "href": "hyperparameter-tuning.html#model-tuning",
    "title": "hyperparameter-tuning",
    "section": "Model Tuning",
    "text": "Model Tuning\n\n1. Build a model for your chosen specification\n\nlibrary(parsnip)\nlibrary(tune)\n\nenet_model &lt;- linear_reg(\n  mode = \"regression\",\n  penalty = tune(),    \n  mixture = tune()    \n) %&gt;%\n  set_engine(\"glmnet\")\n\n\n\n2. Create a workflow\n\nlibrary(workflows)\n\nenet_workflow &lt;- workflow() %&gt;%\n  add_model(enet_model) %&gt;%\n  add_recipe(camels_recipe)\n\n\n\n3. Check the tunables values/ranges\n\nlibrary(tune)\n\ndials &lt;- extract_parameter_set_dials(enet_workflow)\n\ndials$object\n\n[[1]]\nAmount of Regularization (quantitative)\nTransformer: log-10 [1e-100, Inf]\nRange (transformed scale): [-10, 0]\n\n[[2]]\nProportion of Lasso Penalty (quantitative)\nRange: [0.05, 1]\n\n\n\n\n4. Define the search space\n\nlibrary(dials)\nlibrary(finetune)\n\nWarning: package 'finetune' was built under R version 4.4.3\n\nmy.grid &lt;- grid_space_filling(\n  dials,\n  size = 25\n)\n\nmy.grid\n\n# A tibble: 25 × 2\n    penalty mixture\n      &lt;dbl&gt;   &lt;dbl&gt;\n 1 1   e-10  0.604 \n 2 2.61e-10  0.327 \n 3 6.81e-10  0.842 \n 4 1.78e- 9  0.129 \n 5 4.64e- 9  0.485 \n 6 1.21e- 8  0.683 \n 7 3.16e- 8  0.921 \n 8 8.25e- 8  0.288 \n 9 2.15e- 7  0.0896\n10 5.62e- 7  0.525 \n# ℹ 15 more rows\n\n\n\n\n5. Tune the Model\n\nlibrary(tune)\nlibrary(yardstick)\nlibrary(ggplot2)\n\nmodel_params &lt;- tune_grid(\n  enet_workflow,\n  resamples = camels_folds,\n  grid = my.grid,\n  metrics = metric_set(rmse, rsq, mae),\n  control = control_grid(save_pred = TRUE)\n)\n\nWarning: package 'glmnet' was built under R version 4.4.3\n\nautoplot(model_params)\n\n\n\n\n\n\n\n\n\nAmount of regularization: Most models cluster around very low penalty values and perform well across all metrics. As regularization increases performace drops sharply (RMSE and MAE jump up and R squared falls)\n\n\nProportion of Lasso Penalty: Performace is fairly consistent, but RMSE is lowest and R squared is highest between 0.25 and 0.5\n\n\n\n6. Check the skill of the tuned model\n\ncollect_metrics(model_params)\n\n# A tibble: 75 × 8\n         penalty mixture .metric .estimator  mean     n std_err .config         \n           &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;           \n 1 0.000464       0.05   mae     standard   0.120    10 0.00448 Preprocessor1_M…\n 2 0.000464       0.05   rmse    standard   0.173    10 0.00995 Preprocessor1_M…\n 3 0.000464       0.05   rsq     standard   0.988    10 0.00144 Preprocessor1_M…\n 4 0.000000215    0.0896 mae     standard   0.120    10 0.00430 Preprocessor1_M…\n 5 0.000000215    0.0896 rmse    standard   0.172    10 0.00985 Preprocessor1_M…\n 6 0.000000215    0.0896 rsq     standard   0.988    10 0.00142 Preprocessor1_M…\n 7 0.00000000178  0.129  mae     standard   0.120    10 0.00441 Preprocessor1_M…\n 8 0.00000000178  0.129  rmse    standard   0.173    10 0.0100  Preprocessor1_M…\n 9 0.00000000178  0.129  rsq     standard   0.988    10 0.00138 Preprocessor1_M…\n10 0.0562         0.169  mae     standard   0.120    10 0.00453 Preprocessor1_M…\n# ℹ 65 more rows\n\n\n\ncollect_metrics(model_params) %&gt;%\n  filter(.metric == \"mae\") %&gt;%\n  arrange(mean)\n\n# A tibble: 25 × 8\n         penalty mixture .metric .estimator  mean     n std_err .config         \n           &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;           \n 1 0.00825        0.723  mae     standard   0.110    10 0.00430 Preprocessor1_M…\n 2 0.00316        0.446  mae     standard   0.115    10 0.00405 Preprocessor1_M…\n 3 0.000000215    0.0896 mae     standard   0.120    10 0.00430 Preprocessor1_M…\n 4 0.000464       0.05   mae     standard   0.120    10 0.00448 Preprocessor1_M…\n 5 0.00000000178  0.129  mae     standard   0.120    10 0.00441 Preprocessor1_M…\n 6 0.00121        0.248  mae     standard   0.120    10 0.00421 Preprocessor1_M…\n 7 0.0562         0.169  mae     standard   0.120    10 0.00453 Preprocessor1_M…\n 8 0.00001        0.208  mae     standard   0.121    10 0.00427 Preprocessor1_M…\n 9 0.0000000001   0.604  mae     standard   0.121    10 0.00426 Preprocessor1_M…\n10 0.0000261      0.406  mae     standard   0.121    10 0.00433 Preprocessor1_M…\n# ℹ 15 more rows\n\n\n\nshow_best(model_params, metric = \"mae\")\n\n# A tibble: 5 × 8\n        penalty mixture .metric .estimator  mean     n std_err .config          \n          &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;            \n1 0.00825        0.723  mae     standard   0.110    10 0.00430 Preprocessor1_Mo…\n2 0.00316        0.446  mae     standard   0.115    10 0.00405 Preprocessor1_Mo…\n3 0.000000215    0.0896 mae     standard   0.120    10 0.00430 Preprocessor1_Mo…\n4 0.000464       0.05   mae     standard   0.120    10 0.00448 Preprocessor1_Mo…\n5 0.00000000178  0.129  mae     standard   0.120    10 0.00441 Preprocessor1_Mo…\n\n\n\nThe lowest MAE is achieved when penalty is .00825 and a mixture of lasso and ridhe is 0.72 (72% lass and 28% ridge)\n\nhp_best &lt;- select_best(model_params, metric = \"mae\")\n\n\n\n\n7. Finalize your model\n\nfinal_enet_workflow &lt;- finalize_workflow(\n  enet_workflow,\n  hp_best\n)"
  },
  {
    "objectID": "hyperparameter-tuning.html#final-model-verification",
    "href": "hyperparameter-tuning.html#final-model-verification",
    "title": "hyperparameter-tuning",
    "section": "Final Model Verification",
    "text": "Final Model Verification\n\nfinal_fit &lt;- last_fit(\n  final_enet_workflow,\n  split = camels_split\n)\n\n\ncollect_metrics(final_fit)\n\n# A tibble: 2 × 4\n  .metric .estimator .estimate .config             \n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard       0.165 Preprocessor1_Model1\n2 rsq     standard       0.987 Preprocessor1_Model1\n\n\n\nThe RSME indicates that the models predictions are 0.16 units from the mean, while the rsq indicates %98.7 of the variation in the mean is explained by the predictors. The test performace is very close to the training performance.\n\nfinal_predictions &lt;- collect_predictions(final_fit)\n\n\nlibrary(ggplot2)\n\nggplot(final_predictions, aes(x = .pred, y = .pred)) +\n  geom_point(alpha = 0.6, color = \"#3182bd\") +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"#de2d26\", linetype = \"dashed\") +\n  geom_abline(intercept = 0, slope = 1, color = \"#31a354\", linetype = \"solid\") +\n  labs(\n    title = \"Predicted vs. Actual on Test Data\",\n    x = \"Predicted Values\",\n    y = \"Actual Values\"\n  ) +\n  theme_minimal()\n\n`geom_smooth()` using formula = 'y ~ x'"
  },
  {
    "objectID": "hyperparameter-tuning.html#building-a-map",
    "href": "hyperparameter-tuning.html#building-a-map",
    "title": "hyperparameter-tuning",
    "section": "Building a Map",
    "text": "Building a Map\n\nfinal_fit &lt;- fit(final_enet_workflow, data = camels_clean)\n\n\nlibrary(broom)\nfull_predictions &lt;- augment(final_fit, new_data = camels_clean)\n\n\nfull_predictions &lt;- full_predictions %&gt;%\n  mutate(residuals = (.pred - q_mean)^2)\n\n\nlibrary(ggplot2)\nlibrary(patchwork)\n\nmap_pred &lt;- ggplot(full_predictions, aes(x = gauge_lon, y = gauge_lat, color = .pred)) +\n  geom_point(size = 1.5) +\n  scale_color_viridis_c(name = \"Prediction\") +\n  theme_minimal() +\n  labs(title = \"Predicted Streamflow (q_mean)\", x = \"Longitude\", y = \"Latitude\")\n\nmap_pred\n\n\n\n\n\n\n\n\n\nmap_resid &lt;- ggplot(full_predictions, aes(x = gauge_lon, y = gauge_lat, color = residuals)) +\n  geom_point(size = 1.5) +\n  scale_color_viridis_c(name = \"Residuals\") +\n  theme_minimal() +\n  labs(title = \"Squared Residuals\", x = \"Longitude\", y = \"Latitude\")\n\nmap_resid\n\n\n\n\n\n\n\n\n\nlibrary(patchwork)\n\nmap_pred + map_resid"
  }
]